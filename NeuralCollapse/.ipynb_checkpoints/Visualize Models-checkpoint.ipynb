{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eead10ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from pytorch_grad_cam import GradCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from torchvision.models import resnet18\n",
    "import pickle\n",
    "from torchvision import models\n",
    "from torchvision import datasets, transforms\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a65850a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = pickle.load(open(\"configs/CIFAR100_Resnet50.p\", \"rb\"))\n",
    "\n",
    "\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0e991b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(path):\n",
    "    model = eval(f\"models.{conf['model_conf']['model_name']}(pretrained=False, num_classes={conf['C']})\")\n",
    "#     model.conv1 = nn.Conv2d(conf['input_ch'], model.conv1.weight.shape[0], 3, 1, 1,\n",
    "#             bias=False)  # Small dataset filter size used by He et al. (2015)\n",
    "\n",
    "#     model.maxpool = nn.MaxPool2d(kernel_size=1, stride=1, padding=0)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    target_layers = [model.layer3[-1]]\n",
    "    cam = GradCAM(model=model, target_layers=target_layers, use_cuda=True)\n",
    "\n",
    "    return model, cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bea7aec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = \"models/MNIST_RESNET18_model_custom_loss.h\"\n",
    "path_1 = \"models/CIFAR100_resnet50_False.h\"\n",
    "model_1, cam_1 = load_model(path_1)\n",
    "\n",
    "# path_2 = \"models/MNIST_RESNET18_model_custom_loss.h\"\n",
    "# model_2, cam_2 = load_model(path_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7cd802ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.Pad((conf['padded_im_size'] - conf['im_size'])//2),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.4914, 0.4822, 0.4466), (0.2470, 0.2435, 0.2616))])\n",
    "real_dataset = eval(f'datasets.{conf[\"dataset\"]}(\"../data\", train=False, download=True)')\n",
    "test_dataset = eval(f'datasets.{conf[\"dataset\"]}(\"../data\", train=False, download=True, transform=transform)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "02f79210",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_example(item, cam):\n",
    "    target_category = item[1]\n",
    "#     target_category = 7\n",
    "    input_tensor = item[0].unsqueeze(0)\n",
    "    grayscale_cam = cam(input_tensor=input_tensor, target_category=target_category)\n",
    "\n",
    "    # In this example grayscale_cam has only one image in the batch:\n",
    "    grayscale_cam = grayscale_cam[0, :]\n",
    "    input = item[0].numpy()\n",
    "    input_min, input_max = input.min(), input.max()\n",
    "    input = (input - input_min)/(input_max - input_min).squeeze()\n",
    "    \n",
    "    # R = np.zeros((32,32, 3))\n",
    "    # R[:,:,0] = input\n",
    "    print(f\"input:{input.shape}, grayscale_cam:{grayscale_cam.shape}\")\n",
    "    cam_image = show_cam_on_image(input.reshape(32, 32, 3), grayscale_cam, use_rgb=True)\n",
    "    print(cam_image.shape)\n",
    "    cam_image = cam_image[:,:,::-1]\n",
    "    plt.imshow(cam_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2fac241f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:(3, 32, 32), grayscale_cam:(1, 32, 32)\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.5.4) /tmp/pip-req-build-kv0l0wqx/opencv/modules/imgproc/src/colormap.cpp:736: error: (-5:Bad argument) cv::ColorMap only supports source images of type CV_8UC1 or CV_8UC3 in function 'operator()'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_371227/361928877.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvis_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m96\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcam_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_371227/2397667152.py\u001b[0m in \u001b[0;36mvis_example\u001b[0;34m(item, cam)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# R[:,:,0] = input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"input:{input.shape}, grayscale_cam:{grayscale_cam.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mcam_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshow_cam_on_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrayscale_cam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_rgb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcam_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mcam_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcam_image\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch_main/lib/python3.9/site-packages/pytorch_grad_cam/utils/image.py\u001b[0m in \u001b[0;36mshow_cam_on_image\u001b[0;34m(img, mask, use_rgb, colormap)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mreturns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcam\u001b[0m \u001b[0moverlay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \"\"\"\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mheatmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplyColorMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolormap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muse_rgb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mheatmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.5.4) /tmp/pip-req-build-kv0l0wqx/opencv/modules/imgproc/src/colormap.cpp:736: error: (-5:Bad argument) cv::ColorMap only supports source images of type CV_8UC1 or CV_8UC3 in function 'operator()'\n"
     ]
    }
   ],
   "source": [
    "vis_example(test_dataset[96], cam_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7aba0703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=32x32 at 0x7FA4847695B0>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_dataset[96][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "10d5cf9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "real_dataset = datasets.CIFAR100(\"../data\", train=False, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9d18d2f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_dataset[0][0].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c4244b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33881e13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
